# llm-reflexion
## LLM Planning related blogs

1. Reflection Agents (https://blog.langchain.dev/reflection-agents/)
2. Reflexion (https://www.promptingguide.ai/techniques/reflexion)
3. Can LLM critique and iterate over it's own outcome? (https://evjang.com/2023/03/26/self-reflection.html)
4. Agentic Design Pattern - Reflexion (https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/)
5. Introspective Agents: Performing Tasks With Reflection(https://docs.llamaindex.ai/en/latest/examples/agent/introspective_agent_toxicity_reduction/)
6. Reinforcement Learning from Reflective Feedback (RLRF)(https://arxiv.org/abs/2403.14238)
7. When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models (https://arxiv.org/abs/2404.09129)
8. Thought of Search: Planning with Language Models Through The Lens of Efficiency(https://arxiv.org/abs/2404.11833)
9. Automating Thought of Search: A Journey Towards Soundness and Completeness(https://arxiv.org/abs/2408.11326)



## LLM Reflexion related papers

1. Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models (https://arxiv.org/pdf/2310.04406)
2. Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies (https://arxiv.org/pdf/2310.04406)
3. Language Agents with Verbal Reinforcement Learning (https://arxiv.org/abs/2303.11366)
4.RoT: Enhancing Large Language Models with Reflection on Search Trees (https://arxiv.org/abs/2404.05449)
5. Self-Refine: Iterative Refinement with Self-Feedback (https://arxiv.org/abs/2303.17651)
6. CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing (https://arxiv.org/abs/2303.11366)
7. Deceiving to Enlighten: Coaxing LLMs to Self-Reflection for Enhanced Bias Detection and Mitigation
(https://arxiv.org/html/2404.10160v1)


## Reasoning
1. Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models(https://arxiv.org/abs/2406.04271)
2. Graph of Thoughts(https://arxiv.org/abs/2308.09687)
3. Monte Carlo Tree Search (MCTS)(https://arxiv.org/abs/2406.07394)
4. STaR: Self-Taught Reasoner(STaR) - Bootstrap Reasoning with Reasoning(https://arxiv.org/pdf/2203.14465)
5. CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization(CLIN)(https://arxiv.org/abs/2310.10134)
6. Self Discover(SELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures)(https://arxiv.org/pdf/2402.03620)
7. Control Theory for LLM Prompting (https://arxiv.org/abs/2310.04444)
8. Demystifying Chains, Trees, and Graphs of Thoughts(https://arxiv.org/html/2401.14295v3)

# ol-related

1. Cobbe, Karl, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,
Lukasz Kaiser, Matthias Plappert, et al. 2021. “Training Verifiers to
Solve Math Word Problems.” *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2110.14168>.

2. Gandhi, Kanishk, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng,
Archit Sharma, and Noah D Goodman. 2024. “Stream of Search (SoS):
Learning to Search in Language.” *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2404.03683>.

3. Kazemnejad, Amirhossein, Milad Aghajohari, Eva Portelance, Alessandro
Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. 2024.
“VinePPO: Unlocking RL Potential for LLM Reasoning Through Refined
Credit Assignment.” *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2410.01679>.

4. Kirchner, Jan Hendrik, Yining Chen, Harri Edwards, Jan Leike, Nat
McAleese, and Yuri Burda. 2024. “Prover-Verifier Games Improve
Legibility of LLM Outputs.” *arXiv \[Cs.CL\]*.
<http://arxiv.org/abs/2407.13692>.

5. Kumar, Aviral, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes,
Avi Singh, Kate Baumli, et al. 2024. “Training Language Models to
Sel f-Correct via Reinforcement Learning.” *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2409.12917>.

6. Lightman, Hunter, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen
Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl
Cobbe. 2023. “Let’s Verify Step by Step.” *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2305.20050>.

7. Snell, Charlie, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. “Scaling
LLM Test-Time Compute Optimally Can Be More Effective Than Scaling Model
Parameters.” *arXiv \[Cs.LG\]*. <http://arxiv.org/abs/2408.03314>.

8. Uesato, Jonathan, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel,
Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022.
“Solving Math Word Problems with Process- and Outcome-Based Feedback.”
*arXiv \[Cs.LG\]*. <http://arxiv.org/abs/2211.14275>.

9. Wang, Junlin, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. 2024.
“Mixture-of-Agents Enhances Large Language Model Capabilities.” *arXiv
\[Cs.CL\]*. <http://arxiv.org/abs/2406.04692>.

10. Wang, Xuezhi, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan
Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. “Self-Consistency
Improves Chain of Thought Reasoning in Language Models.” *arXiv
\[Cs.CL\]*. <http://arxiv.org/abs/2203.11171>.

11. Wu, Tianhao, Janice Lan, Weizhe Yuan, Jiantao Jiao, Jason Weston, and
Sainbayar Sukhbaatar. 2024. “Thinking LLMs: General Instruction
Following with Thought Generation.” *arXiv \[Cs.CL\]*.
<http://arxiv.org/abs/2410.10630>.

12. Wu, Yangzhen, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang.
2024. “Inference Scaling Laws: An Empirical Analysis of Compute-Optimal
Inference for Problem-Solving with Language Models.” *arXiv \[Cs.AI\]*.
<http://arxiv.org/abs/2408.00724>.

13. Yoshida, Davis, Kartik Goyal, and Kevin Gimpel. 2024.
“<span class="nocase">MAP’s</span> Not Dead yet: Uncovering True
Language Model Modes by Conditioning Away Degeneracy.” In *Proceedings
of the 62nd Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers)*, 16164–215. Stroudsburg, PA, USA:
Association for Computational Linguistics.
<https://aclanthology.org/2024.acl-long.855.pdf>.

14. Zelikman, Eric, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber,
and Noah D Goodman. 2024. “Quiet-STaR: Language Models Can Teach
Themselves to Think Before Speaking.” *arXiv \[Cs.CL\]*.
<http://arxiv.org/abs/2403.09629>.

15. Zelikman, Eric, Yuhuai Wu, Jesse Mu, and Noah D Goodman. 2022. “STaR:
Bootstrapping Reasoning with Reasoning.” *arXiv \[Cs.LG\]*.
<http://arxiv.org/abs/2203.14465>.

## Others
1. Microsoft GraphRAG (https://github.com/microsoft/graphrag)
2. OpenAI Q* - (https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/)




